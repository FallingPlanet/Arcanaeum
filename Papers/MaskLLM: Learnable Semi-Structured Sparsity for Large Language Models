MaskLLM: Learnable Semi-Structured Sparsity for Large Language Models
Authors: Gongfan Fang, Hongxu Yin, Saurav Muralidharan, Greg Heinrich, Jeff Pool, Jan Kautz, Pavlo Molchanov, Xinchao Wang
Affiliations: NVIDIA, National University of Singapore
Source: arXiv:2409.17481v1 [cs.AI], September 26, 2024
Repository: GitHub - NVlabs/MaskLLM

Abstract
MaskLLM introduces a novel learnable pruning technique that imposes semi-structured (N
) sparsity on Large Language Models (LLMs), significantly reducing computational and memory overhead during inference. By modeling mask selection as a learnable distribution using Gumbel Softmax sampling, MaskLLM enables end-to-end training on extensive datasets, achieving high-quality sparsity masks and facilitating transfer learning across various tasks and domains. Evaluated on models ranging from 843M to 15B parameters (e.g., LLaMA-2, Nemotron-4, GPT-3), MaskLLM outperforms state-of-the-art pruning methods, maintaining lower perplexity (e.g., 6.72 PPL vs. 10+ PPL) without updating model weights. Additionally, MaskLLM enables lossless compression for downstream tasks, offering up to 1.4× speedup and 73% memory reduction.

Key Contributions
Learnable N
Sparsity: Introduces a method to learn mask patterns within fixed N
sparsity constraints using probabilistic modeling and differentiable sampling.
Gumbel Softmax Integration: Utilizes Gumbel Softmax to enable differentiable mask sampling, allowing gradient-based optimization of mask probabilities.
Scalability and Transferability: Demonstrates effective scaling to large datasets and models, with the ability to transfer learned sparsity masks across different tasks and domains via MaskPrior.
Empirical Superiority: Achieves lower perplexity and better performance metrics compared to existing pruning techniques like SparseGPT and Wanda across multiple LLM architectures.
Practical Efficiency Gains: Provides significant inference speedups and memory savings, making large models more deployable in resource-constrained environments.
Methodology
Semi-Structured Pruning (N
Sparsity): Enforces that within every group of M consecutive parameters, exactly N are non-zero, optimizing for hardware-friendly sparsity patterns.
Probabilistic Mask Learning: Frames mask selection as a sampling process from a learned categorical distribution, optimizing mask probabilities to minimize language modeling loss.
End-to-End Training: Conducts joint optimization of mask distributions and model performance on large-scale datasets without altering the original model weights.
MaskPrior for Transfer Learning: Initializes mask distributions using pre-computed masks from one-shot pruning methods to enhance training efficiency and mask quality for new tasks.
Experimental Results
Model Evaluation: Applied MaskLLM to LLaMA-2 (7B, 13B), Nemotron-4 (15B), and GPT-3 variants (843M, 2B), achieving superior perplexity scores and task-specific performances compared to baselines.
Scalability: Demonstrated improved mask quality with increased training samples, maintaining effectiveness even with up to 512k samples.
Transfer Learning: Successfully adapted general sparsity masks to specific downstream tasks, achieving lossless compression and maintaining model accuracy.
Efficiency Metrics: Achieved 1.4× inference speedup and 73% memory reduction with 2:4 sparsity, validated across various tasks and domains.
Conclusion
MaskLLM presents an advanced, scalable approach to imposing semi-structured sparsity on large language models through learnable mask patterns. By leveraging differentiable sampling and end-to-end training, it achieves significant reductions in computational and memory requirements while maintaining or enhancing model performance. Its ability to transfer sparsity patterns across tasks further underscores its utility for deploying LLMs in diverse, resource-constrained real-world applications.

Practical Implications
Enhanced Deployment: Enables efficient deployment of large-scale models in environments with limited computational resources.
Flexible Adaptation: Facilitates customization of models for specific tasks without the need for multiple model copies.
Hardware Compatibility: Aligns sparsity patterns with existing GPU architectures, ensuring practical acceleration benefits.
Recommendation:
MaskLLM is a pivotal advancement in model pruning techniques for large language models, offering both theoretical innovations and practical benefits. It is highly recommended for researchers and practitioners focused on optimizing LLMs for efficiency and deployment in real-world scenarios.

Now that the obviously GPT Generated Summary is done, Given some arbitrary finetuned LLM for domain applications, I assume we can perform the End-to-End training on the specific dataset, potentially we would integrate
A dataset like FineWeb or a similar general dataset, or we would test perplexity on those post-masking. Hopefully this method has the potential of providing speedups and memory reduction for domain specific tasks and removes necessity for smaller models
